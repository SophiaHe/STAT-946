{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2544778694765536128\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 169476096\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 10438161854521502838\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:87:00.0\"\n",
      ", name: \"/gpu:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 169476096\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 14348223712715794654\n",
      "physical_device_desc: \"device: 1, name: Tesla K80, pci bus id: 0000:8c:00.0\"\n",
      ", name: \"/gpu:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 171573248\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 5792057993565770289\n",
      "physical_device_desc: \"device: 2, name: Tesla K80, pci bus id: 0000:8d:00.0\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6,7\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU \n",
    "from keras import backend as K\n",
    "\n",
    "with open('./train_data', 'rb') as f: \n",
    "    train_data = pickle.load(f) \n",
    "    train_label= pickle.load(f)\n",
    "with open('./test_data', 'rb') as f: \n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # WRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6,7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import pickle as pickle\n",
    "# import wide_residual_network as wrn\n",
    "import wide_residual_network10 as wrn\n",
    "from keras.datasets import cifar10\n",
    "import keras.callbacks as callbacks\n",
    "import keras.utils.np_utils as kutils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "with open('./train_data', 'rb') as f: \n",
    "    train_data = pickle.load(f) \n",
    "    train_label= pickle.load(f)\n",
    "with open('./test_data', 'rb') as f: \n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (50000, 32, 32, 3)\n",
      "(45000, 100) (50000, 10)\n",
      "(5000, 100) (10000, 10)\n",
      "(5000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# trainX = training[:45000, 0:3072].reshape(45000, 32, 32, 3)\n",
    "# trainX.shape\n",
    "# trainY.shape\n",
    "print(trainX.shape, a.shape)\n",
    "print(trainY.shape, b.shape)\n",
    "print(validY.shape, d.shape)\n",
    "print(validX.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (45000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "(10000, 32, 32, 3) (45000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "(45000, 100) (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.reshape(train_data,(50000,32,32,3))\n",
    "# print(X_train.shape)\n",
    "\n",
    "y_train = np.ravel(train_label)\n",
    "# print(len(y_train))\n",
    "# len(np.unique(y_train))\n",
    "\n",
    "training = np.append(train_data, y_train.reshape(50000,1), 1)\n",
    "\n",
    "trainX = training[:45000, 0:3072].reshape(45000, 32, 32, 3)\n",
    "trainY = training[:45000, -1]\n",
    "validX = training[45000:, 0:3072].reshape(5000, 32, 32, 3)\n",
    "validY = training[45000:, -1]\n",
    "\n",
    "testX = np.reshape(test_data,(10000,32,32,3))\n",
    "print(testX.shape, trainX.shape, validX.shape)\n",
    "\n",
    "# normalization\n",
    "trainX = trainX.astype('float32')\n",
    "trainX = (trainX - trainX.mean(axis=0)) / (trainX.std(axis=0))\n",
    "validX = validX.astype('float32')\n",
    "validX = (validX - validX.mean(axis=0)) / (validX.std(axis=0))\n",
    "print(testX.shape, trainX.shape, validX.shape)\n",
    "\n",
    "trainY = kutils.to_categorical(trainY)\n",
    "validY = kutils.to_categorical(validY)\n",
    "print(trainY.shape, validY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-10-8 created.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 16)    432         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 32, 16)    64          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 16)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 32, 128)   18432       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 128)   512         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 128)   0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 128)   147456      activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 32, 32, 128)   2048        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 32, 32, 128)   0           conv2d_3[0][0]                   \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 32, 32, 128)   512         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 128)   0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 32, 32, 128)   147456      activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 32, 32, 128)   512         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 128)   0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 32, 32, 128)   147456      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 32, 32, 128)   0           add_1[0][0]                      \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 32, 32, 128)   512         add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 32, 32, 128)   0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 16, 16, 256)   294912      activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 16, 16, 256)   1024        conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 16, 16, 256)   0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 16, 16, 256)   589824      activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 16, 16, 256)   32768       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 16, 16, 256)   0           conv2d_8[0][0]                   \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 16, 16, 256)   1024        add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 16, 16, 256)   0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 16, 16, 256)   589824      activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16, 16, 256)   1024        conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 16, 16, 256)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 16, 16, 256)   589824      activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 16, 16, 256)   0           add_3[0][0]                      \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 16, 16, 256)   1024        add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 16, 16, 256)   0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 8, 8, 512)     1179648     activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 8, 8, 512)     2048        conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 8, 8, 512)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 8, 8, 512)     2359296     activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 8, 8, 512)     131072      activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 8, 8, 512)     0           conv2d_13[0][0]                  \n",
      "                                                                   conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 8, 8, 512)     2048        add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 8, 8, 512)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 8, 8, 512)     2359296     activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 8, 8, 512)     2048        conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 8, 8, 512)     0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 8, 8, 512)     2359296     activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 8, 8, 512)     0           add_5[0][0]                      \n",
      "                                                                   conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 8, 8, 512)     2048        add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 8, 8, 512)     0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           51300       flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 11,014,740\n",
      "Trainable params: 11,007,540\n",
      "Non-trainable params: 7,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "nb_epoch = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "\n",
    "generator = ImageDataGenerator(rotation_range=10,\n",
    "                               width_shift_range=5./32,\n",
    "                               height_shift_range=5./32,)\n",
    "\n",
    "init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "\n",
    "# For WRN-16-8 put N = 2, k = 8\n",
    "# For WRN-28-10 put N = 4, k = 10\n",
    "# For WRN-40-4 put N = 6, k = 4\n",
    "model = wrn.create_wide_residual_network(init_shape, nb_classes=100, N=2, k=8, dropout=0.00)\n",
    "\n",
    "model.summary()\n",
    "# plot_model(model, \"WRN-16-8.png\", show_shapes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling\n",
      "Epoch 1/10\n",
      "450/450 [==============================] - 361s - loss: 4.2132 - acc: 0.0521   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:405: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 355s - loss: 3.6035 - acc: 0.1421   \n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 355s - loss: 3.1900 - acc: 0.2134   \n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 354s - loss: 2.9318 - acc: 0.2661   \n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 352s - loss: 2.7169 - acc: 0.3043   \n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 353s - loss: 2.5446 - acc: 0.3421   \n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 352s - loss: 2.3994 - acc: 0.3730   \n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 352s - loss: 2.2694 - acc: 0.3990   \n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 351s - loss: 2.1450 - acc: 0.4258   \n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 352s - loss: 2.0357 - acc: 0.4499   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92e4ec6c88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "# model.load_weights(\"weights/WRN-16-8 Weights.h5\")\n",
    "# print(\"Model loaded.\")\n",
    "\n",
    "# model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "#                    callbacks=[callbacks.ModelCheckpoint(\"./WRN-16-8 Weights.h5\",\n",
    "#                                                         monitor=\"val_acc\",\n",
    "#                                                         save_best_only=True,\n",
    "#                                                         verbose=1)],\n",
    "#                    validation_data=(validX, validY),\n",
    "#                    validation_steps=validX.shape[0] // batch_size,)\n",
    "\n",
    "# model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "#                    callbacks=None,\n",
    "#                    validation_data=(validX, validY),\n",
    "#                    validation_steps=validX.shape[0] // batch_size,)\n",
    "model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "                   callbacks=[callbacks.ModelCheckpoint(\"./WRN-16-8 Weights.h5\",\n",
    "                                                        monitor=\"val_acc\",\n",
    "                                                        save_best_only=True,\n",
    "                                                        verbose=1)])\n",
    "\n",
    "# yPreds = model.predict(validX)\n",
    "# yPred = np.argmax(yPreds, axis=1)\n",
    "# yPred = kutils.to_categorical(yPred)\n",
    "# yTrue = validY\n",
    "\n",
    "# accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
    "# error = 100 - accuracy\n",
    "# print(\"Accuracy : \", accuracy)\n",
    "# print(\"Error : \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 39 39 39 39 39 39 39 39 39]\n"
     ]
    }
   ],
   "source": [
    "prd_wrn = model.predict(testX)\n",
    "prd_wrn_y = np.argmax(prd_wrn, axis=1)\n",
    "print(prd_wrn_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"946_wrn.csv\", prd_wrn_y, delimiter=\",\") # yield 0.35 on kaggle\n",
    "model.save('./946_wrn.h5') # creates a HDF5 file 'CNN1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-10-8 created.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 16)    432         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 32, 16)    64          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 16)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 32, 128)   18432       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 128)   512         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 128)   0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 128)   147456      activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 32, 32, 128)   2048        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 32, 32, 128)   0           conv2d_3[0][0]                   \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 32, 32, 128)   512         add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 128)   0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 32, 32, 128)   147456      activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 32, 32, 128)   512         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 128)   0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 32, 32, 128)   147456      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 32, 32, 128)   0           add_1[0][0]                      \n",
      "                                                                   conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 32, 32, 128)   512         add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 32, 32, 128)   0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 16, 16, 256)   294912      activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 16, 16, 256)   1024        conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 16, 16, 256)   0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 16, 16, 256)   589824      activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 16, 16, 256)   32768       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 16, 16, 256)   0           conv2d_8[0][0]                   \n",
      "                                                                   conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 16, 16, 256)   1024        add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 16, 16, 256)   0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 16, 16, 256)   589824      activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16, 16, 256)   1024        conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 16, 16, 256)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 16, 16, 256)   589824      activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 16, 16, 256)   0           add_3[0][0]                      \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 16, 16, 256)   1024        add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 16, 16, 256)   0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 8, 8, 512)     1179648     activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 8, 8, 512)     2048        conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 8, 8, 512)     0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 8, 8, 512)     2359296     activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 8, 8, 512)     131072      activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 8, 8, 512)     0           conv2d_13[0][0]                  \n",
      "                                                                   conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 8, 8, 512)     2048        add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 8, 8, 512)     0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 8, 8, 512)     2359296     activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 8, 8, 512)     2048        conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 8, 8, 512)     0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 8, 8, 512)     2359296     activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 8, 8, 512)     0           add_5[0][0]                      \n",
      "                                                                   conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 8, 8, 512)     2048        add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 8, 8, 512)     0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           51300       flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 11,014,740\n",
      "Trainable params: 11,007,540\n",
      "Non-trainable params: 7,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "nb_epoch = 80\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "\n",
    "generator = ImageDataGenerator(rotation_range=10,\n",
    "                               width_shift_range=5./32,\n",
    "                               height_shift_range=5./32,)\n",
    "\n",
    "init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "\n",
    "# For WRN-16-8 put N = 2, k = 8\n",
    "# For WRN-28-10 put N = 4, k = 10\n",
    "# For WRN-40-4 put N = 6, k = 4\n",
    "model = wrn.create_wide_residual_network(init_shape, nb_classes=100, N=2, k=8, dropout=0.00)\n",
    "\n",
    "model.summary()\n",
    "# plot_model(model, \"WRN-16-8.png\", show_shapes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling\n",
      "Epoch 1/80\n",
      "450/450 [==============================] - 373s - loss: 4.2314 - acc: 0.0511   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:405: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/80\n",
      "450/450 [==============================] - 355s - loss: 3.6331 - acc: 0.1374   \n",
      "Epoch 3/80\n",
      "450/450 [==============================] - 354s - loss: 3.1967 - acc: 0.2152   \n",
      "Epoch 4/80\n",
      "450/450 [==============================] - 352s - loss: 2.9280 - acc: 0.2653   \n",
      "Epoch 5/80\n",
      "450/450 [==============================] - 352s - loss: 2.7300 - acc: 0.3049   \n",
      "Epoch 6/80\n",
      "450/450 [==============================] - 351s - loss: 2.5526 - acc: 0.3384   \n",
      "Epoch 7/80\n",
      "450/450 [==============================] - 352s - loss: 2.4089 - acc: 0.3696   \n",
      "Epoch 8/80\n",
      "450/450 [==============================] - 351s - loss: 2.2758 - acc: 0.3985   \n",
      "Epoch 9/80\n",
      "450/450 [==============================] - 350s - loss: 2.1543 - acc: 0.4204   \n",
      "Epoch 10/80\n",
      "450/450 [==============================] - 350s - loss: 2.0434 - acc: 0.4482   \n",
      "Epoch 11/80\n",
      "450/450 [==============================] - 349s - loss: 1.9419 - acc: 0.4723   \n",
      "Epoch 12/80\n",
      "450/450 [==============================] - 350s - loss: 1.8466 - acc: 0.4940   \n",
      "Epoch 13/80\n",
      "450/450 [==============================] - 350s - loss: 1.7642 - acc: 0.5121   \n",
      "Epoch 14/80\n",
      "450/450 [==============================] - 350s - loss: 1.6881 - acc: 0.5292   \n",
      "Epoch 15/80\n",
      "450/450 [==============================] - 349s - loss: 1.6060 - acc: 0.5487   \n",
      "Epoch 16/80\n",
      "450/450 [==============================] - 350s - loss: 1.5316 - acc: 0.5698   \n",
      "Epoch 17/80\n",
      "450/450 [==============================] - 350s - loss: 1.4636 - acc: 0.5834   \n",
      "Epoch 18/80\n",
      "450/450 [==============================] - 349s - loss: 1.3951 - acc: 0.5994   \n",
      "Epoch 19/80\n",
      "450/450 [==============================] - 349s - loss: 1.3315 - acc: 0.6163   \n",
      "Epoch 20/80\n",
      "450/450 [==============================] - 350s - loss: 1.2564 - acc: 0.6348   \n",
      "Epoch 21/80\n",
      "450/450 [==============================] - 350s - loss: 1.1997 - acc: 0.6497   \n",
      "Epoch 22/80\n",
      "450/450 [==============================] - 349s - loss: 1.1395 - acc: 0.6647   \n",
      "Epoch 23/80\n",
      "450/450 [==============================] - 349s - loss: 1.0898 - acc: 0.6795   \n",
      "Epoch 24/80\n",
      "450/450 [==============================] - 349s - loss: 1.0220 - acc: 0.6938   \n",
      "Epoch 25/80\n",
      "450/450 [==============================] - 350s - loss: 0.9746 - acc: 0.7073   \n",
      "Epoch 26/80\n",
      "450/450 [==============================] - 349s - loss: 0.9233 - acc: 0.7203   \n",
      "Epoch 27/80\n",
      "450/450 [==============================] - 349s - loss: 0.8652 - acc: 0.7351   \n",
      "Epoch 28/80\n",
      "450/450 [==============================] - 349s - loss: 0.8301 - acc: 0.7467   \n",
      "Epoch 29/80\n",
      "450/450 [==============================] - 349s - loss: 0.7869 - acc: 0.7599   \n",
      "Epoch 30/80\n",
      "450/450 [==============================] - 350s - loss: 0.7324 - acc: 0.7756   \n",
      "Epoch 31/80\n",
      "450/450 [==============================] - 349s - loss: 0.7062 - acc: 0.7810   \n",
      "Epoch 32/80\n",
      "450/450 [==============================] - 350s - loss: 0.6590 - acc: 0.7920   \n",
      "Epoch 33/80\n",
      "450/450 [==============================] - 349s - loss: 0.6193 - acc: 0.8067   \n",
      "Epoch 34/80\n",
      "450/450 [==============================] - 350s - loss: 0.5950 - acc: 0.8106   \n",
      "Epoch 35/80\n",
      "450/450 [==============================] - 349s - loss: 0.5742 - acc: 0.8197   \n",
      "Epoch 36/80\n",
      "450/450 [==============================] - 349s - loss: 0.5386 - acc: 0.8275   \n",
      "Epoch 37/80\n",
      "450/450 [==============================] - 349s - loss: 0.5092 - acc: 0.8380   \n",
      "Epoch 38/80\n",
      "450/450 [==============================] - 348s - loss: 0.4869 - acc: 0.8449   \n",
      "Epoch 39/80\n",
      "450/450 [==============================] - 349s - loss: 0.4594 - acc: 0.8520   \n",
      "Epoch 40/80\n",
      "450/450 [==============================] - 349s - loss: 0.4347 - acc: 0.8621   \n",
      "Epoch 41/80\n",
      "450/450 [==============================] - 349s - loss: 0.4246 - acc: 0.8629   \n",
      "Epoch 42/80\n",
      "450/450 [==============================] - 349s - loss: 0.4040 - acc: 0.8712   \n",
      "Epoch 43/80\n",
      "450/450 [==============================] - 349s - loss: 0.3912 - acc: 0.8752   \n",
      "Epoch 44/80\n",
      "450/450 [==============================] - 349s - loss: 0.3696 - acc: 0.8820   \n",
      "Epoch 45/80\n",
      "450/450 [==============================] - 350s - loss: 0.3599 - acc: 0.8831   \n",
      "Epoch 46/80\n",
      "450/450 [==============================] - 348s - loss: 0.3483 - acc: 0.8872   \n",
      "Epoch 47/80\n",
      "450/450 [==============================] - 350s - loss: 0.3364 - acc: 0.8923   \n",
      "Epoch 48/80\n",
      "450/450 [==============================] - 349s - loss: 0.3128 - acc: 0.8980   \n",
      "Epoch 49/80\n",
      "450/450 [==============================] - 349s - loss: 0.3097 - acc: 0.8996   \n",
      "Epoch 50/80\n",
      "450/450 [==============================] - 348s - loss: 0.3030 - acc: 0.9013   \n",
      "Epoch 51/80\n",
      "450/450 [==============================] - 349s - loss: 0.2962 - acc: 0.9040   \n",
      "Epoch 52/80\n",
      "450/450 [==============================] - 349s - loss: 0.2904 - acc: 0.9054   \n",
      "Epoch 53/80\n",
      "450/450 [==============================] - 349s - loss: 0.2779 - acc: 0.9105   \n",
      "Epoch 54/80\n",
      "450/450 [==============================] - 349s - loss: 0.2607 - acc: 0.9147   \n",
      "Epoch 55/80\n",
      "450/450 [==============================] - 349s - loss: 0.2563 - acc: 0.9185   \n",
      "Epoch 56/80\n",
      "450/450 [==============================] - 348s - loss: 0.2632 - acc: 0.9139   \n",
      "Epoch 57/80\n",
      "450/450 [==============================] - 349s - loss: 0.2468 - acc: 0.9193   \n",
      "Epoch 58/80\n",
      "450/450 [==============================] - 349s - loss: 0.2399 - acc: 0.9216   \n",
      "Epoch 59/80\n",
      "450/450 [==============================] - 349s - loss: 0.2289 - acc: 0.9266   \n",
      "Epoch 60/80\n",
      "450/450 [==============================] - 348s - loss: 0.2299 - acc: 0.9247   \n",
      "Epoch 61/80\n",
      "450/450 [==============================] - 349s - loss: 0.2297 - acc: 0.9242   \n",
      "Epoch 62/80\n",
      "450/450 [==============================] - 349s - loss: 0.2126 - acc: 0.9300   \n",
      "Epoch 63/80\n",
      "450/450 [==============================] - 349s - loss: 0.2179 - acc: 0.9284   \n",
      "Epoch 64/80\n",
      "450/450 [==============================] - 349s - loss: 0.2091 - acc: 0.9317   \n",
      "Epoch 65/80\n",
      "450/450 [==============================] - 349s - loss: 0.2024 - acc: 0.9338   \n",
      "Epoch 66/80\n",
      "450/450 [==============================] - 349s - loss: 0.2046 - acc: 0.9330   \n",
      "Epoch 67/80\n",
      "450/450 [==============================] - 349s - loss: 0.1890 - acc: 0.9391   \n",
      "Epoch 68/80\n",
      "450/450 [==============================] - 349s - loss: 0.1865 - acc: 0.9400   \n",
      "Epoch 69/80\n",
      "450/450 [==============================] - 349s - loss: 0.1983 - acc: 0.9358   \n",
      "Epoch 70/80\n",
      "450/450 [==============================] - 349s - loss: 0.1853 - acc: 0.9387   \n",
      "Epoch 71/80\n",
      "450/450 [==============================] - 349s - loss: 0.1825 - acc: 0.9418   \n",
      "Epoch 72/80\n",
      "450/450 [==============================] - 349s - loss: 0.1842 - acc: 0.9393   \n",
      "Epoch 73/80\n",
      "450/450 [==============================] - 349s - loss: 0.1657 - acc: 0.9450   \n",
      "Epoch 74/80\n",
      "450/450 [==============================] - 349s - loss: 0.1690 - acc: 0.9450   \n",
      "Epoch 75/80\n",
      "450/450 [==============================] - 348s - loss: 0.1808 - acc: 0.9417   \n",
      "Epoch 76/80\n",
      "450/450 [==============================] - 349s - loss: 0.1676 - acc: 0.9445   \n",
      "Epoch 77/80\n",
      "450/450 [==============================] - 349s - loss: 0.1724 - acc: 0.9439   \n",
      "Epoch 78/80\n",
      "450/450 [==============================] - 349s - loss: 0.1583 - acc: 0.9483   \n",
      "Epoch 79/80\n",
      "450/450 [==============================] - 349s - loss: 0.1563 - acc: 0.9488   \n",
      "Epoch 80/80\n",
      "450/450 [==============================] - 349s - loss: 0.1627 - acc: 0.9479   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11fc1f3470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "# model.load_weights(\"weights/WRN-16-8 Weights.h5\")\n",
    "# print(\"Model loaded.\")\n",
    "\n",
    "# model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "#                    callbacks=[callbacks.ModelCheckpoint(\"./WRN-16-8 Weights.h5\",\n",
    "#                                                         monitor=\"val_acc\",\n",
    "#                                                         save_best_only=True,\n",
    "#                                                         verbose=1)],\n",
    "#                    validation_data=(validX, validY),\n",
    "#                    validation_steps=validX.shape[0] // batch_size,)\n",
    "\n",
    "# model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "#                    callbacks=None,\n",
    "#                    validation_data=(validX, validY),\n",
    "#                    validation_steps=validX.shape[0] // batch_size,)\n",
    "model.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size), steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "                   callbacks=[callbacks.ModelCheckpoint(\"./WRN-16-8 Weights.h5\",\n",
    "                                                        monitor=\"val_acc\",\n",
    "                                                        save_best_only=True,\n",
    "                                                        verbose=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX = testX.astype('float32')\n",
    "testX = (testX - testX.mean(axis=0)) / (testX.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"946_wrn2.csv\", prd_wrn1_y, delimiter=\",\") # yield 0.48 on kaggle\n",
    "model.save('./946_wrn2.h5') # creates a HDF5 file '946_wrn2.h5'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
